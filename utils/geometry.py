import torch
from torch.nn import functional as F
import numpy as np
import constants
from utils.imutils import flip_kp
import cv2
"""
Useful geometric operations, e.g. Perspective projection and a differentiable Rodrigues formula
Parts of the code are taken from https://github.com/MandyMo/pytorch_HMR
"""


def transform_torch(pt, center, scale, res, t, rot=0, device=torch.device('cuda')):
    """Transform pixel location to different reference."""
    new_pt = (torch.Tensor([pt[0]-1, pt[1]-1, 1.]).T).to(device).float()
    new_pt = t @ new_pt
    return new_pt[:2].int()+1

def j2d_processing_torch(kp, center, scale, r, f, crop_t):
    """Process gt 2D keypoints and apply all augmentation transforms."""
    nparts = kp.shape[0]
    for i in range(nparts):
        kp[i, 0:2] = transform_torch(kp[i, 0:2] + 1, center, scale,
                               [constants.IMG_RES, constants.IMG_RES], crop_t, rot=r)
    # convert to normalized coordinates
    kp[:, :2] = 2. * kp[:, :2] / constants.IMG_RES - 1.
    if f:
        kp = flip_kp(kp)
    kp = kp.float()
    return kp



def cam_crop2full(crop_cam, center, scale, full_img_shape, focal_length):
    """convert the camera parameters from the crop camera to the full camera.
    :param crop_cam: shape=(N, 3) weak perspective camera in cropped
       img coordinates (s, tx, ty)
    :param center: shape=(N, 2) bbox coordinates (c_x, c_y)
    :param scale: shape=(N, 1) square bbox resolution  (b / 200)
    :param full_img_shape: shape=(N, 2) original image height and width
    :param focal_length: shape=(N,)
    :return:
    """
    img_h, img_w = full_img_shape[:, 0], full_img_shape[:, 1]
    cx, cy, b = center[:, 0], center[:, 1], scale * 200.
    w_2, h_2 = img_w / 2., img_h / 2.
    # print(b[:5],crop_cam[:5,0])
    bs = b * crop_cam[:, 0] + 1e-9
    tz = 2 * focal_length / bs
    tx = (2 * (cx - w_2) / bs) + crop_cam[:, 1]
    ty = (2 * (cy - h_2) / bs) + crop_cam[:, 2]
    full_cam = torch.stack([tx, ty, tz], dim=-1)
    return full_cam

def cam_crop2full_np(crop_cam, center, scale, full_img_shape, focal_length):
    """convert the camera parameters from the crop camera to the full camera.
    :param crop_cam: shape=(N, 3) weak perspective camera in cropped
       img coordinates (s, tx, ty)
    :param center: shape=(N, 2) bbox coordinates (c_x, c_y)
    :param scale: shape=(N, 1) square bbox resolution  (b / 200)
    :param full_img_shape: shape=(N, 2) original image height and width
    :param focal_length: shape=(N,)
    :return:
    """
    img_h, img_w = full_img_shape[:, 0], full_img_shape[:, 1]
    cx, cy, b = center[:, 0], center[:, 1], scale * 200.
    w_2, h_2 = img_w / 2., img_h / 2.
    # print(b[:5],crop_cam[:5,0])
    bs = b * crop_cam[:, 0] + 1e-9
    tz = 2 * focal_length / bs
    tx = (2 * (cx - w_2) / bs) + crop_cam[:, 1]
    ty = (2 * (cy - h_2) / bs) + crop_cam[:, 2]
    full_cam = np.stack([tx, ty, tz], axis=-1)
    return full_cam


def batch_rodrigues(theta):
    """Convert axis-angle representation to rotation matrix.
    Args:
        theta: size = [B, 3]
    Returns:
        Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]
    """
    l1norm = torch.norm(theta + 1e-8, p = 2, dim = 1)
    angle = torch.unsqueeze(l1norm, -1)
    normalized = torch.div(theta, angle)
    angle = angle * 0.5
    v_cos = torch.cos(angle)
    v_sin = torch.sin(angle)
    quat = torch.cat([v_cos, v_sin * normalized], dim = 1)
    return quat_to_rotmat(quat)

def quat_to_rotmat(quat):
    """Convert quaternion coefficients to rotation matrix.
    Args:
        quat: size = [B, 4] 4 <===>(w, x, y, z)
    Returns:
        Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]
    """ 
    norm_quat = quat
    norm_quat = norm_quat/norm_quat.norm(p=2, dim=1, keepdim=True)
    w, x, y, z = norm_quat[:,0], norm_quat[:,1], norm_quat[:,2], norm_quat[:,3]

    B = quat.size(0)

    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)
    wx, wy, wz = w*x, w*y, w*z
    xy, xz, yz = x*y, x*z, y*z

    rotMat = torch.stack([w2 + x2 - y2 - z2, 2*xy - 2*wz, 2*wy + 2*xz,
                          2*wz + 2*xy, w2 - x2 + y2 - z2, 2*yz - 2*wx,
                          2*xz - 2*wy, 2*wx + 2*yz, w2 - x2 - y2 + z2], dim=1).view(B, 3, 3)
    return rotMat    

def rot6d_to_rotmat(x):
    """Convert 6D rotation representation to 3x3 rotation matrix.
    Based on Zhou et al., "On the Continuity of Rotation Representations in Neural Networks", CVPR 2019
    Input:
        (B,6) Batch of 6-D rotation representations
    Output:
        (B,3,3) Batch of corresponding rotation matrices
    """
    x = x.reshape(-1,3,2)
    a1 = x[:, :, 0]
    a2 = x[:, :, 1]
    b1 = F.normalize(a1)
    b2 = F.normalize(a2 - torch.einsum('bi,bi->b', b1, a2).unsqueeze(-1) * b1)
    b3 = torch.cross(b1, b2)
    return torch.stack((b1, b2, b3), dim=-1)
     
def perspective_projection(points, rotation, translation,
                           focal_length, camera_center):
    """
    This function computes the perspective projection of a set of points.
    Input:
        points (bs, N, 3): 3D points
        rotation (bs, 3, 3): Camera rotation
        translation (bs, 3): Camera translation
        focal_length (bs,) or scalar: Focal length
        camera_center (bs, 2): Camera center
    """
    batch_size = points.shape[0]
    K = torch.zeros([batch_size, 3, 3], device=points.device)
    K[:,0,0] = focal_length
    K[:,1,1] = focal_length
    K[:,2,2] = 1.
    K[:,:-1, -1] = camera_center

    # Transform points
    points = torch.einsum('bij,bkj->bki', rotation, points)
    points = points + translation.unsqueeze(1)

    # Apply perspective distortion
    projected_points = points / points[:,:,-1].unsqueeze(-1)

    # Apply camera intrinsics
    projected_points = torch.einsum('bij,bkj->bki', K, projected_points)

    return projected_points[:, :, :-1]


def estimate_translation_np(S, joints_2d, joints_conf, focal_length=5000, img_size=224):
    """Find camera translation that brings 3D joints S closest to 2D the corresponding joints_2d.
    Input:
        S: (25, 3) 3D joint locations
        joints: (25, 3) 2D joint locations and confidence
    Returns:
        (3,) camera translation vector
    """

    num_joints = S.shape[0]
    # focal length
    f = np.array([focal_length,focal_length])
    # optical center
    center = np.array([img_size/2., img_size/2.])

    # transformations
    Z = np.reshape(np.tile(S[:,2],(2,1)).T,-1)
    XY = np.reshape(S[:,0:2],-1)
    O = np.tile(center,num_joints)
    F = np.tile(f,num_joints)
    weight2 = np.reshape(np.tile(np.sqrt(joints_conf),(2,1)).T,-1)

    # least squares
    Q = np.array([F*np.tile(np.array([1,0]),num_joints), F*np.tile(np.array([0,1]),num_joints), O-np.reshape(joints_2d,-1)]).T
    c = (np.reshape(joints_2d,-1)-O)*Z - F*XY

    # weighted least squares
    W = np.diagflat(weight2)
    Q = np.dot(W,Q)
    c = np.dot(W,c)

    # square matrix
    A = np.dot(Q.T,Q)
    b = np.dot(Q.T,c)

    # solution
    trans = np.linalg.solve(A, b)

    return trans


def estimate_translation(S, joints_2d, focal_length=5000., img_size=224.):
    """Find camera translation that brings 3D joints S closest to 2D the corresponding joints_2d.
    Input:
        S: (B, 49, 3) 3D joint locations
        joints: (B, 49, 3) 2D joint locations and confidence
    Returns:
        (B, 3) camera translation vectors
    """

    device = S.device
    # Use only joints 25:49 (GT joints)
    S = S[:, 25:, :].cpu().numpy()
    joints_2d = joints_2d[:, 25:, :].cpu().numpy()
    joints_conf = joints_2d[:, :, -1]
    joints_2d = joints_2d[:, :, :-1]
    trans = np.zeros((S.shape[0], 3), dtype=np.float32)
    # Find the translation for each example in the batch
    for i in range(S.shape[0]):
        S_i = S[i]
        joints_i = joints_2d[i]
        conf_i = joints_conf[i]
        focal_length_i = focal_length[i]
        trans[i] = estimate_translation_np(S_i, joints_i, conf_i, focal_length=focal_length_i, img_size=img_size)
    return torch.from_numpy(trans).to(device)

def get_global_orient(pose, body_yaw, cam_pitch, cam_yaw, cam_roll):
    # World coordinate transformation after assuming camera has 0 yaw and is at origin
    # print(body_yaw, cam_yaw, cam_pitch)
    body_rotmat, _ = cv2.Rodrigues(np.array([[0, ((body_yaw-90+cam_yaw) / 180) * np.pi, 0]], dtype=float))
    pitch_rotmat, _ = cv2.Rodrigues(np.array([cam_pitch / 180 * np.pi, 0, 0]).reshape(3, 1))
    roll_rotmat, _ = cv2.Rodrigues(np.array([0., 0, cam_roll / 180 * np.pi, ]).reshape(3, 1))
    final_rotmat = np.matmul(roll_rotmat, (pitch_rotmat))
    
    transform_coordinate = np.array([[1.0, 0.0, 0.0], [0.0, -1.0, 0.0], [0.0, 0.0, -1.0]])
    transform_body_rotmat = np.matmul(body_rotmat, transform_coordinate)
    w_global_orient = cv2.Rodrigues(np.dot(transform_body_rotmat, cv2.Rodrigues(pose[:3])[0]))[0].T[0]

    c_global_orient = cv2.Rodrigues(np.dot(final_rotmat, cv2.Rodrigues(w_global_orient)[0]))[0].T[0]

    return w_global_orient, c_global_orient, final_rotmat

def get_global_orient_mm(
                        camPitch,
                        camYaw,
                        yawSMPL,
                        globalOrient=None,
                        meanPose=False):
        """Modified from https://github.com/pixelite1201/agora_evaluation/blob/
        master/agora_evaluation/projection.py specific to AGORA.

        Args:
            imgPath: image path
            df: annotation dataframe
            i: frame index
            pNum: person index
            globalOrient: original global orientation
            meanPose: Store True for mean pose from vposer

        Returns:
            globalOrient: rotated global orientation
        """
        # if 'hdri' in imgPath:
        #     camYaw = 0
        #     camPitch = 0

        # elif 'cam00' in imgPath:
        #     camYaw = 135
        #     camPitch = 30
        # elif 'cam01' in imgPath:
        #     camYaw = -135
        #     camPitch = 30
        # elif 'cam02' in imgPath:
        #     camYaw = -45
        #     camPitch = 30
        # elif 'cam03' in imgPath:
        #     camYaw = 45
        #     camPitch = 30
        # elif 'ag2' in imgPath:
        #     camYaw = 0
        #     camPitch = 15
        # else:
        #     camYaw = df.iloc[i]['camYaw']
        #     camPitch = 0

        # if meanPose:
        #     yawSMPL = 0
        # else:
        #     yawSMPL = df.iloc[i]['Yaw'][pNum]

        # scans have a 90deg rotation, but for mean pose from vposer there is
        # no such rotation
        if meanPose:
            rotMat, _ = cv2.Rodrigues(
                np.array([[0, (yawSMPL) / 180 * np.pi, 0]], dtype=float))
        else:
            rotMat, _ = cv2.Rodrigues(
                np.array([[0, ((yawSMPL - 90) / 180) * np.pi, 0]],
                         dtype=float))

        camera_rotationMatrix, _ = cv2.Rodrigues(
            np.array([0, ((-camYaw) / 180) * np.pi, 0]).reshape(3, 1))
        camera_rotationMatrix2, _ = cv2.Rodrigues(
            np.array([camPitch / 180 * np.pi, 0, 0]).reshape(3, 1))

        # flip pose
        R_mod = cv2.Rodrigues(np.array([np.pi, 0, 0]))[0]
        R_root = cv2.Rodrigues(globalOrient.reshape(-1))[0]
        # new_root = R_root.dot(R_mod)
        new_root = R_mod.dot(R_root)
        globalOrient = cv2.Rodrigues(new_root)[0].reshape(3)

        # apply camera matrices
        globalOrient = rotate_global_orient(rotMat, globalOrient)
        globalOrient = rotate_global_orient(camera_rotationMatrix,
                                                 globalOrient)
        globalOrient = rotate_global_orient(camera_rotationMatrix2,
                                                 globalOrient)

        return globalOrient

def rotate_global_orient(rotMat, global_orient):
    """Transform global orientation given rotation matrix.

    Args:
        rotMat: rotation matrix
        global_orient: original global orientation

    Returns:
        new_global_orient: transformed global orientation
    """
    new_global_orient = cv2.Rodrigues(
        np.dot(rotMat,
                cv2.Rodrigues(global_orient.reshape(-1))[0]))[0].T[0]
    return new_global_orient